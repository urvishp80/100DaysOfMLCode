{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0        1    14.23        1.71  2.43               15.6        127   \n",
      "1        1    13.20        1.78  2.14               11.2        100   \n",
      "2        1    13.16        2.36  2.67               18.6        101   \n",
      "3        1    14.37        1.95  2.50               16.8        113   \n",
      "4        1    13.24        2.59  2.87               21.0        118   \n",
      "5        1    14.20        1.76  2.45               15.2        112   \n",
      "6        1    14.39        1.87  2.45               14.6         96   \n",
      "7        1    14.06        2.15  2.61               17.6        121   \n",
      "8        1    14.83        1.64  2.17               14.0         97   \n",
      "9        1    13.86        1.35  2.27               16.0         98   \n",
      "10       1    14.10        2.16  2.30               18.0        105   \n",
      "11       1    14.12        1.48  2.32               16.8         95   \n",
      "12       1    13.75        1.73  2.41               16.0         89   \n",
      "13       1    14.75        1.73  2.39               11.4         91   \n",
      "14       1    14.38        1.87  2.38               12.0        102   \n",
      "15       1    13.63        1.81  2.70               17.2        112   \n",
      "16       1    14.30        1.92  2.72               20.0        120   \n",
      "17       1    13.83        1.57  2.62               20.0        115   \n",
      "18       1    14.19        1.59  2.48               16.5        108   \n",
      "19       1    13.64        3.10  2.56               15.2        116   \n",
      "20       1    14.06        1.63  2.28               16.0        126   \n",
      "21       1    12.93        3.80  2.65               18.6        102   \n",
      "22       1    13.71        1.86  2.36               16.6        101   \n",
      "23       1    12.85        1.60  2.52               17.8         95   \n",
      "24       1    13.50        1.81  2.61               20.0         96   \n",
      "25       1    13.05        2.05  3.22               25.0        124   \n",
      "26       1    13.39        1.77  2.62               16.1         93   \n",
      "27       1    13.30        1.72  2.14               17.0         94   \n",
      "28       1    13.87        1.90  2.80               19.4        107   \n",
      "29       1    14.02        1.68  2.21               16.0         96   \n",
      "..     ...      ...         ...   ...                ...        ...   \n",
      "148      3    13.32        3.24  2.38               21.5         92   \n",
      "149      3    13.08        3.90  2.36               21.5        113   \n",
      "150      3    13.50        3.12  2.62               24.0        123   \n",
      "151      3    12.79        2.67  2.48               22.0        112   \n",
      "152      3    13.11        1.90  2.75               25.5        116   \n",
      "153      3    13.23        3.30  2.28               18.5         98   \n",
      "154      3    12.58        1.29  2.10               20.0        103   \n",
      "155      3    13.17        5.19  2.32               22.0         93   \n",
      "156      3    13.84        4.12  2.38               19.5         89   \n",
      "157      3    12.45        3.03  2.64               27.0         97   \n",
      "158      3    14.34        1.68  2.70               25.0         98   \n",
      "159      3    13.48        1.67  2.64               22.5         89   \n",
      "160      3    12.36        3.83  2.38               21.0         88   \n",
      "161      3    13.69        3.26  2.54               20.0        107   \n",
      "162      3    12.85        3.27  2.58               22.0        106   \n",
      "163      3    12.96        3.45  2.35               18.5        106   \n",
      "164      3    13.78        2.76  2.30               22.0         90   \n",
      "165      3    13.73        4.36  2.26               22.5         88   \n",
      "166      3    13.45        3.70  2.60               23.0        111   \n",
      "167      3    12.82        3.37  2.30               19.5         88   \n",
      "168      3    13.58        2.58  2.69               24.5        105   \n",
      "169      3    13.40        4.60  2.86               25.0        112   \n",
      "170      3    12.20        3.03  2.32               19.0         96   \n",
      "171      3    12.77        2.39  2.28               19.5         86   \n",
      "172      3    14.16        2.51  2.48               20.0         91   \n",
      "173      3    13.71        5.65  2.45               20.5         95   \n",
      "174      3    13.40        3.91  2.48               23.0        102   \n",
      "175      3    13.27        4.28  2.26               20.0        120   \n",
      "176      3    13.17        2.59  2.37               20.0        120   \n",
      "177      3    14.13        4.10  2.74               24.5         96   \n",
      "\n",
      "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0             2.80        3.06                  0.28             2.29   \n",
      "1             2.65        2.76                  0.26             1.28   \n",
      "2             2.80        3.24                  0.30             2.81   \n",
      "3             3.85        3.49                  0.24             2.18   \n",
      "4             2.80        2.69                  0.39             1.82   \n",
      "5             3.27        3.39                  0.34             1.97   \n",
      "6             2.50        2.52                  0.30             1.98   \n",
      "7             2.60        2.51                  0.31             1.25   \n",
      "8             2.80        2.98                  0.29             1.98   \n",
      "9             2.98        3.15                  0.22             1.85   \n",
      "10            2.95        3.32                  0.22             2.38   \n",
      "11            2.20        2.43                  0.26             1.57   \n",
      "12            2.60        2.76                  0.29             1.81   \n",
      "13            3.10        3.69                  0.43             2.81   \n",
      "14            3.30        3.64                  0.29             2.96   \n",
      "15            2.85        2.91                  0.30             1.46   \n",
      "16            2.80        3.14                  0.33             1.97   \n",
      "17            2.95        3.40                  0.40             1.72   \n",
      "18            3.30        3.93                  0.32             1.86   \n",
      "19            2.70        3.03                  0.17             1.66   \n",
      "20            3.00        3.17                  0.24             2.10   \n",
      "21            2.41        2.41                  0.25             1.98   \n",
      "22            2.61        2.88                  0.27             1.69   \n",
      "23            2.48        2.37                  0.26             1.46   \n",
      "24            2.53        2.61                  0.28             1.66   \n",
      "25            2.63        2.68                  0.47             1.92   \n",
      "26            2.85        2.94                  0.34             1.45   \n",
      "27            2.40        2.19                  0.27             1.35   \n",
      "28            2.95        2.97                  0.37             1.76   \n",
      "29            2.65        2.33                  0.26             1.98   \n",
      "..             ...         ...                   ...              ...   \n",
      "148           1.93        0.76                  0.45             1.25   \n",
      "149           1.41        1.39                  0.34             1.14   \n",
      "150           1.40        1.57                  0.22             1.25   \n",
      "151           1.48        1.36                  0.24             1.26   \n",
      "152           2.20        1.28                  0.26             1.56   \n",
      "153           1.80        0.83                  0.61             1.87   \n",
      "154           1.48        0.58                  0.53             1.40   \n",
      "155           1.74        0.63                  0.61             1.55   \n",
      "156           1.80        0.83                  0.48             1.56   \n",
      "157           1.90        0.58                  0.63             1.14   \n",
      "158           2.80        1.31                  0.53             2.70   \n",
      "159           2.60        1.10                  0.52             2.29   \n",
      "160           2.30        0.92                  0.50             1.04   \n",
      "161           1.83        0.56                  0.50             0.80   \n",
      "162           1.65        0.60                  0.60             0.96   \n",
      "163           1.39        0.70                  0.40             0.94   \n",
      "164           1.35        0.68                  0.41             1.03   \n",
      "165           1.28        0.47                  0.52             1.15   \n",
      "166           1.70        0.92                  0.43             1.46   \n",
      "167           1.48        0.66                  0.40             0.97   \n",
      "168           1.55        0.84                  0.39             1.54   \n",
      "169           1.98        0.96                  0.27             1.11   \n",
      "170           1.25        0.49                  0.40             0.73   \n",
      "171           1.39        0.51                  0.48             0.64   \n",
      "172           1.68        0.70                  0.44             1.24   \n",
      "173           1.68        0.61                  0.52             1.06   \n",
      "174           1.80        0.75                  0.43             1.41   \n",
      "175           1.59        0.69                  0.43             1.35   \n",
      "176           1.65        0.68                  0.53             1.46   \n",
      "177           2.05        0.76                  0.56             1.35   \n",
      "\n",
      "     Color intensity   Hue  OD280/OD315  Proline     \n",
      "0           5.640000  1.04         3.92        1065  \n",
      "1           4.380000  1.05         3.40        1050  \n",
      "2           5.680000  1.03         3.17        1185  \n",
      "3           7.800000  0.86         3.45        1480  \n",
      "4           4.320000  1.04         2.93         735  \n",
      "5           6.750000  1.05         2.85        1450  \n",
      "6           5.250000  1.02         3.58        1290  \n",
      "7           5.050000  1.06         3.58        1295  \n",
      "8           5.200000  1.08         2.85        1045  \n",
      "9           7.220000  1.01         3.55        1045  \n",
      "10          5.750000  1.25         3.17        1510  \n",
      "11          5.000000  1.17         2.82        1280  \n",
      "12          5.600000  1.15         2.90        1320  \n",
      "13          5.400000  1.25         2.73        1150  \n",
      "14          7.500000  1.20         3.00        1547  \n",
      "15          7.300000  1.28         2.88        1310  \n",
      "16          6.200000  1.07         2.65        1280  \n",
      "17          6.600000  1.13         2.57        1130  \n",
      "18          8.700000  1.23         2.82        1680  \n",
      "19          5.100000  0.96         3.36         845  \n",
      "20          5.650000  1.09         3.71         780  \n",
      "21          4.500000  1.03         3.52         770  \n",
      "22          3.800000  1.11         4.00        1035  \n",
      "23          3.930000  1.09         3.63        1015  \n",
      "24          3.520000  1.12         3.82         845  \n",
      "25          3.580000  1.13         3.20         830  \n",
      "26          4.800000  0.92         3.22        1195  \n",
      "27          3.950000  1.02         2.77        1285  \n",
      "28          4.500000  1.25         3.40         915  \n",
      "29          4.700000  1.04         3.59        1035  \n",
      "..               ...   ...          ...         ...  \n",
      "148         8.420000  0.55         1.62         650  \n",
      "149         9.400000  0.57         1.33         550  \n",
      "150         8.600000  0.59         1.30         500  \n",
      "151        10.800000  0.48         1.47         480  \n",
      "152         7.100000  0.61         1.33         425  \n",
      "153        10.520000  0.56         1.51         675  \n",
      "154         7.600000  0.58         1.55         640  \n",
      "155         7.900000  0.60         1.48         725  \n",
      "156         9.010000  0.57         1.64         480  \n",
      "157         7.500000  0.67         1.73         880  \n",
      "158        13.000000  0.57         1.96         660  \n",
      "159        11.750000  0.57         1.78         620  \n",
      "160         7.650000  0.56         1.58         520  \n",
      "161         5.880000  0.96         1.82         680  \n",
      "162         5.580000  0.87         2.11         570  \n",
      "163         5.280000  0.68         1.75         675  \n",
      "164         9.580000  0.70         1.68         615  \n",
      "165         6.620000  0.78         1.75         520  \n",
      "166        10.680000  0.85         1.56         695  \n",
      "167        10.260000  0.72         1.75         685  \n",
      "168         8.660000  0.74         1.80         750  \n",
      "169         8.500000  0.67         1.92         630  \n",
      "170         5.500000  0.66         1.83         510  \n",
      "171         9.899999  0.57         1.63         470  \n",
      "172         9.700000  0.62         1.71         660  \n",
      "173         7.700000  0.64         1.74         740  \n",
      "174         7.300000  0.70         1.56         750  \n",
      "175        10.200000  0.59         1.56         835  \n",
      "176         9.300000  0.60         1.62         840  \n",
      "177         9.200000  0.61         1.60         560  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/Winedata.txt\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "y = df['class']\n",
    "df.drop(['class'], 1, inplace=True)\n",
    "X = np.array(df)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler().fit(X)\n",
    "newX=scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urvish/.pyenv/versions/3.6.0/envs/general/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#spliting data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(newX,y,test_size=0.30,random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's quickly import Logistic Regression model from Linear models class in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build simple function to evaluate classifiers. In this function we will use KFold cross validation and cross validation scores to get how the classifier is performing on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.96       0.96       1.         0.95833333]\n",
      "Mean score: 0.976 (+/-0.010)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build another function to use Sklearn.metrics to find out how the classifiers are performing on data by getting the confusion matrix and classification report. These two techniques are really powerfull and can be used to find out the performance of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "1.0\n",
      "Accuracy on testing set:\n",
      "0.9814814814814815\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      1.00      0.97        15\n",
      "          2       1.00      0.95      0.98        21\n",
      "          3       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       0.98      0.98      0.98        54\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 1 20  0]\n",
      " [ 0  0 18]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that Logistic Regression is doing really great on the classifying wines into three different groups based on some features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
