{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0      1    14.23        1.71  2.43               15.6        127   \n",
      "1      1    13.20        1.78  2.14               11.2        100   \n",
      "2      1    13.16        2.36  2.67               18.6        101   \n",
      "3      1    14.37        1.95  2.50               16.8        113   \n",
      "4      1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color intensity   Hue  OD280/OD315  Proline     \n",
      "0             5.64  1.04         3.92        1065  \n",
      "1             4.38  1.05         3.40        1050  \n",
      "2             5.68  1.03         3.17        1185  \n",
      "3             7.80  0.86         3.45        1480  \n",
      "4             4.32  1.04         2.93         735  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/Winedata.txt\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "y = df['class']\n",
    "df.drop(['class'], 1, inplace=True)\n",
    "X = np.array(df)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler().fit(X)\n",
    "newX=scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urvish/.pyenv/versions/3.6.0/envs/general/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#spliting data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(newX,y,test_size=0.30,random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's quickly import Logistic Regression model from Linear models class in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build simple function to evaluate classifiers. In this function we will use KFold cross validation and cross validation scores to get how the classifier is performing on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.96       0.96       1.         0.95833333]\n",
      "Mean score: 0.976 (+/-0.010)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build another function to use Sklearn.metrics to find out how the classifiers are performing on data by getting the confusion matrix and classification report. These two techniques are really powerfull and can be used to find out the performance of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "1.0\n",
      "Accuracy on testing set:\n",
      "0.9814814814814815\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      1.00      0.97        15\n",
      "          2       1.00      0.95      0.98        21\n",
      "          3       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       0.98      0.98      0.98        54\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  0  0]\n",
      " [ 1 20  0]\n",
      " [ 0  0 18]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that Logistic Regression is doing really great on the classifying wines into three different groups based on some features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
